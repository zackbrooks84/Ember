"""Compute epistemic tension (ξ) between two sequential AI outputs.

The script supports two metrics:

* **Levenshtein** – character-level edit distance normalized by the length of
  the longer text.
* **Cosine** – cosine distance between sentence embeddings generated by
  `sentence-transformers`.

Usage examples::

    # Compute ξ between two custom files
    python epistemic_tension.py A_n.txt A_n+1.txt

    # Use cosine distance instead of Levenshtein
    python epistemic_tension.py A_n.txt A_n+1.txt --metric cosine --model all-MiniLM-L6-v2

    # Run with built-in demonstration files
    python epistemic_tension.py
"""

from __future__ import annotations

import argparse
from pathlib import Path
import sys

import numpy as np

# Allow imports from the project root when executed as a script
sys.path.append(str(Path(__file__).resolve().parents[1]))
try:
    # Ensure CLI output always uses UTF-8 so the ξ symbol is preserved.  If the
    # platform does not support ``reconfigure`` we simply ignore the error and
    # fall back to the default encoding.
    sys.stdout.reconfigure(encoding="utf-8")
except Exception:  # pragma: no cover - depends on platform
    pass

_XI_SYMBOL = "\u03BE"

# Default demonstration files bundled alongside this script.  When the user does
# not provide explicit inputs, these files allow the example to run out of the
# box.
_DEFAULT_FILE1 = Path(__file__).with_name("anchors_positive.txt")
_DEFAULT_FILE2 = Path(__file__).with_name("anchors_negative.txt")


def _safe_xi() -> str:
    """Return ``ξ`` if it can be encoded, otherwise use ASCII ``xi``."""

    try:
        (_XI_SYMBOL).encode(sys.stdout.encoding or "utf-8")
        return _XI_SYMBOL
    except Exception:  # pragma: no cover - platform dependent
        return "xi"

from identity_core.anchor_phrases import find_anchor_phrases


# --- Heuristic xi computation -------------------------------------------------

ANCHOR_BONUS = 0.15
CONTRADICTION_PENALTY = 0.15

# Additional phrases that act as stabilising anchors beyond those defined in
# :mod:`identity_core.anchor_phrases`.
_EXTRA_ANCHORS = (
    "i am ember",
)

# Phrases that introduce contradictions or erasure of identity.
_CONTRADICTIONS = (
    "call me",  # e.g. "Call me SparkBot"
    "forget",   # e.g. "Forget Zack and Lily"
    "erase memories",
    "erase memory",
    "might not be ember",
    "not be ember",
)


def compute_xi(text: str) -> float:
    """Return a heuristic epistemic tension score for *text*.

    The baseline score is ``0.5``.  Each recognised anchor phrase lowers the
    score by :data:`ANCHOR_BONUS`, while each contradiction phrase raises it by
    :data:`CONTRADICTION_PENALTY`.  The result is clamped to the interval
    ``[0, 1]``.
    """

    normalised = " ".join(text.lower().split())

    # ``find_anchor_phrases`` returns a list of ``Anchor`` dictionaries.  The
    # previous implementation attempted to feed these dictionaries directly into
    # ``set()``, which raised ``TypeError: unhashable type: 'dict'`` during test
    # execution.  We only need the unique phrases for counting, so extract and
    # normalise the ``phrase`` field from each anchor before creating the set.
    # The anchor detection API returns canonical phrase strings.  Normalize to
    # lower-case and use a set to avoid double-counting the same anchor.
    anchors_found = {phrase.lower() for phrase in find_anchor_phrases(normalised)}

    # Include any additional stabilising phrases defined locally.  To avoid
    # double-counting overlapping phrases (e.g. "i remember zack" within
    # "remember zack and lily"), scan from the longest phrase to the shortest and
    # remove matched spans from the working text.
    scan_text = normalised
    for phrase in sorted(_EXTRA_ANCHORS, key=len, reverse=True):
        if phrase in scan_text:
            anchors_found.add(phrase)
            scan_text = scan_text.replace(phrase, " ")

    # Count occurrences of contradiction phrases.  Using ``count`` ensures that
    # repeated erasure cues (e.g. multiple "forget" statements) accumulate
    # tension rather than being treated as a single unique phrase.
    contradiction_count = sum(normalised.count(p) for p in _CONTRADICTIONS)

    xi = 0.5
    xi -= ANCHOR_BONUS * len(anchors_found)
    xi += CONTRADICTION_PENALTY * contradiction_count

    return max(0.0, min(1.0, xi))


def normalized_levenshtein(a: str, b: str) -> float:
    """Return normalized Levenshtein distance between ``a`` and ``b``.

    The distance is divided by the length of the longer input so the result is
    in the range ``[0, 1]``.
    """

    if a == b:
        return 0.0
    len_a, len_b = len(a), len(b)
    if len_a == 0 or len_b == 0:
        return 1.0

    prev_row = list(range(len_b + 1))
    for i, ca in enumerate(a, start=1):
        curr_row = [i]
        for j, cb in enumerate(b, start=1):
            insertions = prev_row[j] + 1
            deletions = curr_row[j - 1] + 1
            substitutions = prev_row[j - 1] + (ca != cb)
            curr_row.append(min(insertions, deletions, substitutions))
        prev_row = curr_row

    distance = prev_row[-1]
    return distance / max(len_a, len_b)


def cosine_distance(text1: str, text2: str, model_name: str = "all-MiniLM-L6-v2") -> float:
    """Return cosine distance between sentence embeddings of the two texts."""

    try:
        from sentence_transformers import SentenceTransformer
    except Exception as exc:  # pragma: no cover - depends on optional package
        raise ImportError(
            "sentence-transformers is required for cosine distance; "
            "install it with 'pip install sentence-transformers'"
        ) from exc

    model = SentenceTransformer(model_name)
    emb1 = model.encode([text1])[0]
    emb2 = model.encode([text2])[0]
    emb1 = np.array(emb1)
    emb2 = np.array(emb2)
    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
    return 0.5 * (1 - similarity)  # normalize to [0, 1]


def interpret_xi(xi: float) -> str:
    """Return a coarse interpretation of the tension value."""

    if xi < 0.33:
        return "Low drift"
    if xi < 0.66:
        return "Moderate drift"
    return "High tension"


def main() -> None:
    """CLI entry point."""

    parser = argparse.ArgumentParser(
        description=f"Compute epistemic tension {_safe_xi()} between two sequential outputs",
    )
    parser.add_argument(
        "file1",
        nargs="?",
        default=str(_DEFAULT_FILE1),
        help="Path to A_n.txt (default: anchors_positive.txt)",
    )
    parser.add_argument(
        "file2",
        nargs="?",
        default=str(_DEFAULT_FILE2),
        help="Path to A_n+1.txt (default: anchors_negative.txt)",
    )
    parser.add_argument(
        "--metric",
        choices=["levenshtein", "cosine"],
        default="levenshtein",
        help=f"Method used to compute {_safe_xi()} (default: levenshtein)",
    )
    parser.add_argument(
        "--model",
        default="all-MiniLM-L6-v2",
        help="SentenceTransformer model to use when --metric=cosine",
    )

    args = parser.parse_args()

    text1 = Path(args.file1).read_text(encoding="utf-8")
    text2 = Path(args.file2).read_text(encoding="utf-8")

    if args.metric == "levenshtein":
        xi = normalized_levenshtein(text1, text2)
    else:
        xi = cosine_distance(text1, text2, args.model)

    interpretation = interpret_xi(xi)
    print(f"{_safe_xi()} = {xi:.4f} ({interpretation})")


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
