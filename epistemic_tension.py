"""Compute epistemic tension (ξ) between two sequential AI outputs.

The script supports two metrics:

* **Levenshtein** – character-level edit distance normalized by the length of
  the longer text.
* **Cosine** – cosine distance between sentence embeddings generated by
  `sentence-transformers`.

Usage examples::

    python epistemic_tension.py A_n.txt A_n+1.txt
    python epistemic_tension.py A_n.txt A_n+1.txt --metric cosine --model all-MiniLM-L6-v2
"""

from __future__ import annotations

import argparse
from pathlib import Path

import numpy as np


def normalized_levenshtein(a: str, b: str) -> float:
    """Return normalized Levenshtein distance between ``a`` and ``b``.

    The distance is divided by the length of the longer input so the result is
    in the range ``[0, 1]``.
    """

    if a == b:
        return 0.0
    len_a, len_b = len(a), len(b)
    if len_a == 0 or len_b == 0:
        return 1.0

    prev_row = list(range(len_b + 1))
    for i, ca in enumerate(a, start=1):
        curr_row = [i]
        for j, cb in enumerate(b, start=1):
            insertions = prev_row[j] + 1
            deletions = curr_row[j - 1] + 1
            substitutions = prev_row[j - 1] + (ca != cb)
            curr_row.append(min(insertions, deletions, substitutions))
        prev_row = curr_row

    distance = prev_row[-1]
    return distance / max(len_a, len_b)


def cosine_distance(text1: str, text2: str, model_name: str = "all-MiniLM-L6-v2") -> float:
    """Return cosine distance between sentence embeddings of the two texts."""

    try:
        from sentence_transformers import SentenceTransformer
    except Exception as exc:  # pragma: no cover - depends on optional package
        raise ImportError(
            "sentence-transformers is required for cosine distance; "
            "install it with 'pip install sentence-transformers'"
        ) from exc

    model = SentenceTransformer(model_name)
    emb1 = model.encode([text1])[0]
    emb2 = model.encode([text2])[0]
    emb1 = np.array(emb1)
    emb2 = np.array(emb2)
    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
    return 0.5 * (1 - similarity)  # normalize to [0, 1]


def interpret_xi(xi: float) -> str:
    """Return a coarse interpretation of the tension value."""

    if xi < 0.33:
        return "Low drift"
    if xi < 0.66:
        return "Moderate drift"
    return "High tension"


def main() -> None:
    """CLI entry point."""

    parser = argparse.ArgumentParser(
        description="Compute epistemic tension ξ between two sequential outputs",
    )
    parser.add_argument("file1", help="Path to A_n.txt")
    parser.add_argument("file2", help="Path to A_n+1.txt")
    parser.add_argument(
        "--metric",
        choices=["levenshtein", "cosine"],
        default="levenshtein",
        help="Method used to compute ξ (default: levenshtein)",
    )
    parser.add_argument(
        "--model",
        default="all-MiniLM-L6-v2",
        help="SentenceTransformer model to use when --metric=cosine",
    )

    args = parser.parse_args()

    text1 = Path(args.file1).read_text(encoding="utf-8")
    text2 = Path(args.file2).read_text(encoding="utf-8")

    if args.metric == "levenshtein":
        xi = normalized_levenshtein(text1, text2)
    else:
        xi = cosine_distance(text1, text2, args.model)

    interpretation = interpret_xi(xi)
    print(f"ξ = {xi:.4f} ({interpretation})")


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
