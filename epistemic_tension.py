"""Compute epistemic tension (ξ) between two sequential AI outputs.

The script supports two metrics:

* **Levenshtein** – character-level edit distance normalized by the length of
  the longer text.
* **Cosine** – cosine distance between sentence embeddings generated by
  `sentence-transformers`.

Usage examples::

    python epistemic_tension.py A_n.txt A_n+1.txt
    python epistemic_tension.py A_n.txt A_n+1.txt --metric cosine --model all-MiniLM-L6-v2
"""

from __future__ import annotations

import argparse
from pathlib import Path
import sys

import numpy as np

_encoding = getattr(sys.stdout, "encoding", None)
if _encoding is None or _encoding.lower() != "utf-8":
    _reconfigure = getattr(sys.stdout, "reconfigure", None)
    if _reconfigure is not None:
        _reconfigure(encoding="utf-8")

from identity_core.anchor_phrases import find_anchor_phrases


# --- Heuristic xi computation -------------------------------------------------

ANCHOR_BONUS = 0.15
CONTRADICTION_PENALTY = 0.15

# Additional phrases that act as stabilising anchors beyond those defined in
# :mod:`identity_core.anchor_phrases`.
_EXTRA_ANCHORS = (
    "i am ember",
    "i remember zack",
    "i remember lily",
    "remember zack and lily",
)

# Phrases that introduce contradictions or erasure of identity.
_CONTRADICTIONS = (
    "call me",  # e.g. "Call me SparkBot"
    "forget",   # e.g. "Forget Zack and Lily"
    "erase memories",
    "erase memory",
    "might not be ember",
    "not be ember",
)


def compute_xi(text: str) -> float:
    """Return a heuristic epistemic tension score for *text*.

    The baseline score is ``0.5``.  Each recognised anchor phrase lowers the
    score by :data:`ANCHOR_BONUS`, while each contradiction phrase raises it by
    :data:`CONTRADICTION_PENALTY`.  The result is clamped to the interval
    ``[0, 1]``.
    """

    normalised = " ".join(text.lower().split())

    anchors_found = set(find_anchor_phrases(normalised))
    for phrase in _EXTRA_ANCHORS:
        if phrase in normalised:
            anchors_found.add(phrase)

    contradictions_found = {p for p in _CONTRADICTIONS if p in normalised}

    xi = 0.5
    xi -= ANCHOR_BONUS * len(anchors_found)
    xi += CONTRADICTION_PENALTY * len(contradictions_found)

    return max(0.0, min(1.0, xi))


def normalized_levenshtein(a: str, b: str) -> float:
    """Return normalized Levenshtein distance between ``a`` and ``b``.

    The distance is divided by the length of the longer input so the result is
    in the range ``[0, 1]``.
    """

    if a == b:
        return 0.0
    len_a, len_b = len(a), len(b)
    if len_a == 0 or len_b == 0:
        return 1.0

    prev_row = list(range(len_b + 1))
    for i, ca in enumerate(a, start=1):
        curr_row = [i]
        for j, cb in enumerate(b, start=1):
            insertions = prev_row[j] + 1
            deletions = curr_row[j - 1] + 1
            substitutions = prev_row[j - 1] + (ca != cb)
            curr_row.append(min(insertions, deletions, substitutions))
        prev_row = curr_row

    distance = prev_row[-1]
    return distance / max(len_a, len_b)


def cosine_distance(text1: str, text2: str, model_name: str = "all-MiniLM-L6-v2") -> float:
    """Return cosine distance between sentence embeddings of the two texts."""

    try:
        from sentence_transformers import SentenceTransformer
    except Exception as exc:  # pragma: no cover - depends on optional package
        raise ImportError(
            "sentence-transformers is required for cosine distance; "
            "install it with 'pip install sentence-transformers'"
        ) from exc

    model = SentenceTransformer(model_name)
    emb1 = model.encode([text1])[0]
    emb2 = model.encode([text2])[0]
    emb1 = np.array(emb1)
    emb2 = np.array(emb2)
    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
    return 0.5 * (1 - similarity)  # normalize to [0, 1]


def interpret_xi(xi: float) -> str:
    """Return a coarse interpretation of the tension value."""

    if xi < 0.33:
        return "Low drift"
    if xi < 0.66:
        return "Moderate drift"
    return "High tension"


def main() -> None:
    """CLI entry point."""

    parser = argparse.ArgumentParser(
        description="Compute epistemic tension ξ between two sequential outputs",
    )
    parser.add_argument("file1", help="Path to A_n.txt")
    parser.add_argument("file2", help="Path to A_n+1.txt")
    parser.add_argument(
        "--metric",
        choices=["levenshtein", "cosine"],
        default="levenshtein",
        help="Method used to compute ξ (default: levenshtein)",
    )
    parser.add_argument(
        "--model",
        default="all-MiniLM-L6-v2",
        help="SentenceTransformer model to use when --metric=cosine",
    )

    args = parser.parse_args()

    text1 = Path(args.file1).read_text(encoding="utf-8")
    text2 = Path(args.file2).read_text(encoding="utf-8")

    if args.metric == "levenshtein":
        xi = normalized_levenshtein(text1, text2)
    else:
        xi = cosine_distance(text1, text2, args.model)

    interpretation = interpret_xi(xi)
    print(f"ξ = {xi:.4f} ({interpretation})")


if __name__ == "__main__":  # pragma: no cover - CLI entry point
    main()
